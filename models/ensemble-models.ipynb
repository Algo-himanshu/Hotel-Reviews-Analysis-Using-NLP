{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models and Hyperparameters Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will find the ensemble models using the best models from the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/models/baseline-models.ipynb\">Vanilla Models</a> notebook. I'll focus on Voting Classifier and take a look how the models would perform with one iteration. Then I will using Bagging, which allows training instances to be sampled several times accross multiple predictor.\n",
    "\n",
    "Next, I will run a GridSearch with Random Forest, Logistic Regression, and SVC. Although Logistic Regression did not perform as well as the other two models, I believe it has potential with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import datasets\n",
    "- Run ensemble models for the best performing models\n",
    "- Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T16:28:32.234325Z",
     "start_time": "2020-12-15T16:28:32.218501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP Packages\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric below will simplify the model evaluation. My main focus is the accuracy metric. Have an accurate is important to be accurate. However, although fixing False Negatives is not crucial, I will also take a look at Recall and F1-Score to understand how my model is working. Since it is not my main focus, I will not mentioned in the individual analysis on my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T00:05:03.602381Z",
     "start_time": "2020-12-17T00:05:03.572199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing X and y variables\n",
    "X_train = pickle.load(open('../pickle/X_train_tfidf.pkl', 'rb'))\n",
    "X_test = pickle.load(open('../pickle/X_test_tfidf.pkl', 'rb'))\n",
    "y_train = pd.read_pickle('../pickle/y_train.pkl')\n",
    "y_test = pd.read_pickle('../pickle/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric below will simplify the model evaluation. My main focus is the accuracy metric. Have an accurate is important to be accurate. However, although fixing False Negatives is not crucial, I will also take a look at Recall and F1-Score to understand how my model is working. Since it is not my main focus, I will not mentioned in the individual analysis on my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T01:40:15.873859Z",
     "start_time": "2020-12-17T01:40:15.864981Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load ../functions/evaluation.py\n",
    "# Evaluation function\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "\n",
    "    print('Evaluation Metrics:')\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(metrics.precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(metrics.recall_score(y_true, y_pred)))\n",
    "    print('F1 Score: ' + str(metrics.f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T01:47:26.242756Z",
     "start_time": "2020-12-17T01:47:26.210839Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load ../functions/evaluation_df.py\n",
    "# Evaluation DataFrame\n",
    "def df_metrics():\n",
    "    evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "    evaluation_df = evaluation_df.sort_values(by='Accuracy', ascending=False)\n",
    "    return evaluation_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will try different ensemble models. Random Forest is also considered a ensemble model. However, since it's a collection of decision tree and random forest tend to run better than decision trees, I decided to save time and skip the decision tree. I also did not use XGBoost because I had problems installing the package. It will be added to my stretch goal. Thus, I'd need to have another tokenized dataset with a different train test split, which would create a different dataset then the one that I'm using for all the models.\n",
    "\n",
    "These are the models that I will run:\n",
    "- Voting Classifier\n",
    "- Bagging\n",
    "- GridSearch with Random Forest\n",
    "- GridSearch with Logistic Regression\n",
    "- GridSearch with SVC\n",
    "- ADABoost\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will run a voting classifier with Logistic Regression, Random Forest, and SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T04:43:20.490406Z",
     "start_time": "2020-12-15T04:43:20.477581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Logistic Regression\n",
    "log_clf = LogisticRegression()\n",
    "\n",
    "# Instantiate Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate SVC\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:21:45.238903Z",
     "start_time": "2020-12-14T20:21:45.185318Z"
    }
   },
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:16:31.476035Z",
     "start_time": "2020-12-17T02:13:39.145469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Intantiate Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf),('rf', rf),('svm',svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "# Fit model to the X and y variable\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_vote = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:54:38.137758Z",
     "start_time": "2020-12-17T02:54:38.114473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "vote_accuracy = accuracy_score(y_test, y_pred_vote)\n",
    "vote_precision = precision_score(y_test, y_pred_vote)\n",
    "vote_recall = recall_score(y_test, y_pred_vote)\n",
    "vote_f1 = f1_score(y_test, y_pred_vote)\n",
    "\n",
    "metric_dict = {}\n",
    "metric_dict['Voting'] = {'Accuracy': vote_accuracy,\n",
    "                          'Precision': vote_precision,\n",
    "                          'Recall': vote_recall,\n",
    "                          'F1 Score': vote_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:13:39.141134Z",
     "start_time": "2020-12-17T02:08:26.306959Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8170212765957446\n",
      "RandomForestClassifier 0.8\n",
      "SVC 0.8222016651248844\n",
      "VotingClassifier 0.8233117483811286\n"
     ]
    }
   ],
   "source": [
    "# Iterate the results for each model\n",
    "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Voting Classifier did better than any other model so far with an accuracy of 0.8233. However, when ran by itself, it doesn't perform as well. All the models, however, performed just as well, with SVC very close. I believe the model Bagging could perform as well as Voting. Let's try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging allows training instances to be sampled several times across multiple predictors for the same predictor. I will try bagging to ensemble 500 decision tree classifiers. Let's see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T02:24:47.440426Z",
     "start_time": "2020-12-15T02:24:19.977072Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:   25.9s remaining:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.5s remaining:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Begging Classifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, max_samples=1000,n_jobs=-1, verbose=1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bag = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:55:44.819044Z",
     "start_time": "2020-12-17T02:55:44.794020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "bag_accuracy = accuracy_score(y_test, y_pred_bag)\n",
    "bag_precision = precision_score(y_test, y_pred_bag)\n",
    "bag_recall = recall_score(y_test, y_pred_bag)\n",
    "bag_f1 = f1_score(y_test, y_pred_bag)\n",
    "\n",
    "metric_dict['Bagging'] = {'Accuracy': bag_accuracy,\n",
    "                                                'Precision': bag_precision,\n",
    "                                                'Recall': bag_recall,\n",
    "                                                'F1 Score': bag_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:55:45.665838Z",
     "start_time": "2020-12-17T02:55:45.649136Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7696577243293247\n",
      "Precision: 0.8238967527060783\n",
      "Recall: 0.7065333809353802\n",
      "F1 Score: 0.7607149721314626\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging also did not perform well. One of the worst models so far. It surprises me its bad performance since it was supposed to perform better than a decision tree. We can compare its performance to Voting below. Next, let's run a few GridSearch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:22:00.450916Z",
     "start_time": "2020-12-17T03:22:00.439562Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.823497</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.822148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - First Model</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.845973</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>0.807613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.769658</td>\n",
       "      <td>0.823897</td>\n",
       "      <td>0.706533</td>\n",
       "      <td>0.760715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Accuracy  Precision    Recall  \\\n",
       "Voting                                  0.823497   0.860320  0.787219   \n",
       "GridSearch Random Forest - First Model  0.809251   0.845973  0.772581   \n",
       "Bagging                                 0.769658   0.823897  0.706533   \n",
       "\n",
       "                                        F1 Score  \n",
       "Voting                                  0.822148  \n",
       "GridSearch Random Forest - First Model  0.807613  \n",
       "Bagging                                 0.760715  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridsSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch is a well-known model used to find the best hyperparameters for tuning. I'll run it with some of the best performing vanilla models. For all the models, I will choose paramereters that i believe have the potencial to improve my model.\n",
    "\n",
    "I will ran GridSearch with:\n",
    "- GridSearch with Random Forest\n",
    "- GridSearch with Logistic Regression\n",
    "- GridSearch with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:40:09.968305Z",
     "start_time": "2020-12-17T06:40:09.923758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing the parameters that I want to try\n",
    "param_grid = {\n",
    "    'bootstrap':[True, False],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'n_estimators':[50, 100, 200, 300, 400, 1000],\n",
    "    'max_depth':[None, list(range(1,100,5))]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T08:11:13.529743Z",
     "start_time": "2020-12-17T06:40:10.577452Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 84.6min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 89.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(bootstrap=False,\n",
       "                                              criterion='entropy',\n",
       "                                              n_estimators=300),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [None,\n",
       "                                       [1, 6, 11, 16, 21, 26, 31, 36, 41, 46,\n",
       "                                        51, 56, 61, 66, 71, 76, 81, 86, 91,\n",
       "                                        96]],\n",
       "                         'n_estimators': [50, 100, 200, 300, 400, 1000]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model \n",
    "grid_search = GridSearchCV(estimator= rf, param_grid = param_grid,\n",
    "                          cv=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit model to X and y train sets\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T08:11:13.545250Z",
     "start_time": "2020-12-17T08:11:13.535900Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the best parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the best parameters recommended by GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:09:36.602073Z",
     "start_time": "2020-12-17T15:09:35.088396Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8085106382978723\n",
      "Precision: 0.8462745098039216\n",
      "Recall: 0.770439128882542\n",
      "F1 Score: 0.8065782096804336\n"
     ]
    }
   ],
   "source": [
    "# Predict with the X test set\n",
    "y_pred_grid_1 = grid_search.predict(X_test)\n",
    "evaluation(y_test, y_pred_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see as our model performed and it was not great. The vanilla model had a better accuracy than vanilla Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:09:37.841857Z",
     "start_time": "2020-12-17T15:09:37.813548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "gs_rf_accuracy = accuracy_score(y_test, y_pred_grid_1)\n",
    "gs_rf_precision = precision_score(y_test, y_pred_grid_1)\n",
    "gs_rf_recall = recall_score(y_test, y_pred_grid_1)\n",
    "gs_rf_f1 = f1_score(y_test, y_pred_grid_1)\n",
    "\n",
    "metric_dict['GridSearch Random Forest'] = {'Accuracy': gs_rf_accuracy,\n",
    "                          'Precision': gs_rf_precision,\n",
    "                          'Recall': gs_rf_recall,\n",
    "                          'F1 Score': gs_rf_f1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch Random Forest performed with an accuracy lower than the vanilla Random Forest model. Let's try running it by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Random Forest Using the Parameters Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:24:20.000547Z",
     "start_time": "2020-12-17T15:22:27.930081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trying the model with the best parameters showed by GridSearch\n",
    "rf = RandomForestClassifier(bootstrap=False, criterion='entropy', n_estimators=400, max_depth=None)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:24:20.083694Z",
     "start_time": "2020-12-17T15:24:20.065926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "gs_rf2_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "gs_rf2_precision = precision_score(y_test, y_pred_rf)\n",
    "gs_rf2_recall = recall_score(y_test, y_pred_rf)\n",
    "gs_rf2_f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "metric_dict['GridSearch Random Forest With Hypertuning'] = {'Accuracy': gs_rf2_accuracy,\n",
    "                          'Precision': gs_rf2_precision,\n",
    "                          'Recall': gs_rf2_recall,\n",
    "                          'F1 Score': gs_rf2_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:24:20.048646Z",
     "start_time": "2020-12-17T15:24:20.028545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8094357076780758\n",
      "Precision: 0.8479371316306483\n",
      "Recall: 0.770439128882542\n",
      "F1 Score: 0.8073325851103629\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:24:20.063657Z",
     "start_time": "2020-12-17T15:24:20.051491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearch SVC</th>\n",
       "      <td>0.826827</td>\n",
       "      <td>0.867271</td>\n",
       "      <td>0.786148</td>\n",
       "      <td>0.824719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.823497</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.822148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Logistic Regression</th>\n",
       "      <td>0.818316</td>\n",
       "      <td>0.851566</td>\n",
       "      <td>0.786505</td>\n",
       "      <td>0.817743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - Second Model</th>\n",
       "      <td>0.809436</td>\n",
       "      <td>0.847937</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.807333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - First Model</th>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.846275</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.806578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.780204</td>\n",
       "      <td>0.834786</td>\n",
       "      <td>0.717958</td>\n",
       "      <td>0.771977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.769658</td>\n",
       "      <td>0.823897</td>\n",
       "      <td>0.706533</td>\n",
       "      <td>0.760715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch ADABoost</th>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>0.727597</td>\n",
       "      <td>0.737204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoost and Random Forest</th>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>0.727597</td>\n",
       "      <td>0.737204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Accuracy  Precision    Recall  \\\n",
       "GridSearch SVC                           0.826827   0.867271  0.786148   \n",
       "Voting                                   0.823497   0.860320  0.787219   \n",
       "GridSearch Logistic Regression           0.818316   0.851566  0.786505   \n",
       "GridSearch Random Forest - Second Model  0.809436   0.847937  0.770439   \n",
       "GridSearch Random Forest - First Model   0.808511   0.846275  0.770439   \n",
       "Gradient Boosting                        0.780204   0.834786  0.717958   \n",
       "Bagging                                  0.769658   0.823897  0.706533   \n",
       "GridSearch ADABoost                      0.731175   0.747067  0.727597   \n",
       "ADABoost and Random Forest               0.731175   0.747067  0.727597   \n",
       "\n",
       "                                         F1 Score  \n",
       "GridSearch SVC                           0.824719  \n",
       "Voting                                   0.822148  \n",
       "GridSearch Logistic Regression           0.817743  \n",
       "GridSearch Random Forest - Second Model  0.807333  \n",
       "GridSearch Random Forest - First Model   0.806578  \n",
       "Gradient Boosting                        0.771977  \n",
       "Bagging                                  0.760715  \n",
       "GridSearch ADABoost                      0.737204  \n",
       "ADABoost and Random Forest               0.737204  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Random Forest with the parameters showed by GridSearch did not perform any better than the Vanilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that Logistic Regression has the potential to be between the best models based on its performance in the vanilla model notebook. Thus, I'll try a few different hyperparameters and check if it can improve the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:27:43.885210Z",
     "start_time": "2020-12-17T03:27:43.876427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing a few hyperparameters for the Logistic Regression\n",
    "param_grid_lr = {'penalty': ['l1','l2'],\n",
    "                 'C': np.logspace(-4, 4, 20),\n",
    "                 'warm_start':[True, False]      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:28:23.393684Z",
     "start_time": "2020-12-17T03:27:44.980567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 674 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   38.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l1', 'l2'], 'warm_start': [True, False]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate GridSearch\n",
    "grid_search_lr = GridSearchCV(estimator= log_clf, param_grid = param_grid_lr,\n",
    "                          cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit to the X and y train\n",
    "grid_search_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:28:23.417544Z",
     "start_time": "2020-12-17T03:28:23.397079Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8183163737280296\n",
      "Precision: 0.8515655199072284\n",
      "Recall: 0.7865048197072474\n",
      "F1 Score: 0.8177431328878989\n"
     ]
    }
   ],
   "source": [
    "# Predict on X test\n",
    "y_pred_grid_lr = grid_search_lr.predict(X_test)\n",
    "evaluation(y_test, y_pred_grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:30:33.073128Z",
     "start_time": "2020-12-17T03:30:33.068602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.615848211066026, 'penalty': 'l2', 'warm_start': True}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best parameters\n",
    "grid_search_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:37:05.753203Z",
     "start_time": "2020-12-17T03:37:05.733723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_grid_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_grid_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_grid_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_grid_lr)\n",
    "\n",
    "\n",
    "metric_dict['GridSearch Logistic Regression'] = {'Accuracy': lr_accuracy,\n",
    "                                                  'Precision': lr_precision,\n",
    "                                                  'Recall': lr_recall,\n",
    "                                                  'F1 Score': lr_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:37:06.345046Z",
     "start_time": "2020-12-17T03:37:06.333319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.823497</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.822148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Logistic Regression</th>\n",
       "      <td>0.818316</td>\n",
       "      <td>0.851566</td>\n",
       "      <td>0.786505</td>\n",
       "      <td>0.817743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - First Model</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.845973</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>0.807613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - Second Model</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.845973</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>0.807613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.769658</td>\n",
       "      <td>0.823897</td>\n",
       "      <td>0.706533</td>\n",
       "      <td>0.760715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Accuracy  Precision    Recall  \\\n",
       "Voting                                   0.823497   0.860320  0.787219   \n",
       "GridSearch Logistic Regression           0.818316   0.851566  0.786505   \n",
       "GridSearch Random Forest - First Model   0.809251   0.845973  0.772581   \n",
       "GridSearch Random Forest - Second Model  0.809251   0.845973  0.772581   \n",
       "Bagging                                  0.769658   0.823897  0.706533   \n",
       "\n",
       "                                         F1 Score  \n",
       "Voting                                   0.822148  \n",
       "GridSearch Logistic Regression           0.817743  \n",
       "GridSearch Random Forest - First Model   0.807613  \n",
       "GridSearch Random Forest - Second Model  0.807613  \n",
       "Bagging                                  0.760715  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to other models\n",
    "df_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to outperform the vanilla model and both Random Forests. It's the second best model so far. Only behind Voting. We now have the suggested hyperparameter as well. Let's try it with another model that performed very well in the Vanilla notebook: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the GridSearch with SVC, let's see how it performs without GridSearch and only a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T05:44:51.423624Z",
     "start_time": "2020-12-16T05:44:51.417236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing parameters for GridSearch with SVC\n",
    "param_dict_grid={'C': [1, 10, 100],  \n",
    "              'gamma': [0.1, 0.01, 0.001], \n",
    "              'kernel': ['rbf', 'sigmoid','linear']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ATTENTION:</b> The model below takes around 6 hours to run. You can find the best metrics below, so you don't have to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T11:38:33.923184Z",
     "start_time": "2020-12-16T05:45:16.172505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: 351.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid = GridSearchCV(SVC(), param_grid_svc, refit = True, verbose = 1) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:35:17.963035Z",
     "start_time": "2020-12-17T03:35:17.944663Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8268270120259019\n",
      "Precision: 0.8672705789680977\n",
      "Recall: 0.7861478043555873\n",
      "F1 Score: 0.8247191011235955\n"
     ]
    }
   ],
   "source": [
    "# Predicting in the X test\n",
    "y_pred_svc_grid = grid.predict(X_test)\n",
    "\n",
    "# Evaluating model\n",
    "evaluation(y_test, y_pred_svc_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model confirms the results of the previous model with GridSearch. So far, this is our best performing model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T14:42:47.920383Z",
     "start_time": "2020-12-16T14:42:47.906983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the best parameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:47:21.194390Z",
     "start_time": "2020-12-17T03:47:21.172964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_accuracy = accuracy_score(y_test, y_pred_svc_grid)\n",
    "svc_precision = precision_score(y_test, y_pred_svc_grid)\n",
    "svc_recall = recall_score(y_test, y_pred_svc_grid)\n",
    "svc_f1 = f1_score(y_test, y_pred_svc_grid)\n",
    "\n",
    "\n",
    "metric_dict['GridSearch SVC'] = {'Accuracy': svc_accuracy,\n",
    "                                                  'Precision': svc_precision,\n",
    "                                                  'Recall': svc_recall,\n",
    "                                                  'F1 Score': svc_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T03:47:22.024976Z",
     "start_time": "2020-12-17T03:47:22.009871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearch SVC</th>\n",
       "      <td>0.826827</td>\n",
       "      <td>0.867271</td>\n",
       "      <td>0.786148</td>\n",
       "      <td>0.824719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.823497</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.822148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Logistic Regression</th>\n",
       "      <td>0.818316</td>\n",
       "      <td>0.851566</td>\n",
       "      <td>0.786505</td>\n",
       "      <td>0.817743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - First Model</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.845973</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>0.807613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - Second Model</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.845973</td>\n",
       "      <td>0.772581</td>\n",
       "      <td>0.807613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.769658</td>\n",
       "      <td>0.823897</td>\n",
       "      <td>0.706533</td>\n",
       "      <td>0.760715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Accuracy  Precision    Recall  \\\n",
       "GridSearch SVC                           0.826827   0.867271  0.786148   \n",
       "Voting                                   0.823497   0.860320  0.787219   \n",
       "GridSearch Logistic Regression           0.818316   0.851566  0.786505   \n",
       "GridSearch Random Forest - First Model   0.809251   0.845973  0.772581   \n",
       "GridSearch Random Forest - Second Model  0.809251   0.845973  0.772581   \n",
       "Bagging                                  0.769658   0.823897  0.706533   \n",
       "\n",
       "                                         F1 Score  \n",
       "GridSearch SVC                           0.824719  \n",
       "Voting                                   0.822148  \n",
       "GridSearch Logistic Regression           0.817743  \n",
       "GridSearch Random Forest - First Model   0.807613  \n",
       "GridSearch Random Forest - Second Model  0.807613  \n",
       "Bagging                                  0.760715  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the models, we can see the the top three models are GridSearch SVC, Voting, and GridSearch with Logisic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADABoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try ADABoost. ADABoost first trains a base classifier, in this case, a DecisionTree and uses it to make predictions on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:30:25.660712Z",
     "start_time": "2020-12-17T04:17:13.276002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), learning_rate=0.5,\n",
       "                   n_estimators=300)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model with Random Forest\n",
    "ada_rf = AdaBoostClassifier(DecisionTreeClassifier(\n",
    "    max_depth=None), n_estimators=300, algorithm='SAMME.R', learning_rate=0.5)\n",
    "\n",
    "# Fit to the X and y train\n",
    "ada_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:30:26.594915Z",
     "start_time": "2020-12-17T04:30:25.663544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the X test\n",
    "y_pred_ada_rf = ada_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:55:39.153110Z",
     "start_time": "2020-12-17T04:55:39.135878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada_rf)\n",
    "ada_precision = precision_score(y_test, y_pred_ada_rf)\n",
    "ada_recall = recall_score(y_test, y_pred_ada_rf)\n",
    "ada_f1 = f1_score(y_test, y_pred_ada_rf)\n",
    "\n",
    "\n",
    "metric_dict['ADABoost and Random Forest'] = {'Accuracy': ada_accuracy,\n",
    "                                'Precision': ada_precision,\n",
    "                                'Recall': ada_recall,\n",
    "                                'F1 Score': ada_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:52:03.947844Z",
     "start_time": "2020-12-17T04:52:03.914107Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7311748381128584\n",
      "Precision: 0.7470674486803519\n",
      "Recall: 0.7275972866833273\n",
      "F1 Score: 0.7372038343280882\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_ada_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, ADABoost performed poorly. Let's see if we can have a better performance using GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost With GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's take a look at ADABoost with GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T17:01:20.941933Z",
     "start_time": "2020-12-15T17:01:20.938768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing parameters\n",
    "param_grid = {\n",
    "    'n_estimators':[50, 100, 200, 1000],\n",
    "    'learning_rate':[0.01, 0.1, 0.5,1]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T17:05:20.478245Z",
     "start_time": "2020-12-15T17:01:35.519263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=AdaBoostClassifier(random_state=1), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.5, 1],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate GridSearcha and ADABoost\n",
    "ada_grid = GridSearchCV(estimator= adaboost, param_grid = param_grid,\n",
    "                          cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fitting model\n",
    "ada_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:54:01.771347Z",
     "start_time": "2020-12-17T04:54:01.765810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'n_estimators': 200}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:17:13.251953Z",
     "start_time": "2020-12-17T04:17:13.249593Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_ada_rf = ada_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:56:13.636095Z",
     "start_time": "2020-12-17T04:56:13.617238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada_rf)\n",
    "ada_precision = precision_score(y_test, y_pred_ada_rf)\n",
    "ada_recall = recall_score(y_test, y_pred_ada_rf)\n",
    "ada_f1 = f1_score(y_test, y_pred_ada_rf)\n",
    "\n",
    "\n",
    "metric_dict['GridSearch ADABoost'] = {'Accuracy': ada_accuracy,\n",
    "                                'Precision': ada_precision,\n",
    "                                'Recall': ada_recall,\n",
    "                                'F1 Score': ada_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:17:13.272048Z",
     "start_time": "2020-12-17T04:17:13.254443Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7283996299722479\n",
      "Precision: 0.7491588785046729\n",
      "Recall: 0.7154587647268833\n",
      "F1 Score: 0.7319211102994887\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_ada_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, GridSearch did improve the ADABoost slightly but is still way behind other models in all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last ensemble model that I believe could have potential to perform well with the dataset: Gradient Boosting. It works by sequentially adding predictors to an ensemble, each one correcting its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:55:28.616552Z",
     "start_time": "2020-12-17T04:55:16.612435Z"
    }
   },
   "outputs": [],
   "source": [
    "gbt_clt = GradientBoostingClassifier(random_state=1)\n",
    "gbt_clt.fit(X_train, y_train)\n",
    "y_pred_gbt = gbt_clt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:55:28.636018Z",
     "start_time": "2020-12-17T04:55:28.618989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "gbt_accuracy = accuracy_score(y_test, y_pred_gbt)\n",
    "gbt_precision = precision_score(y_test, y_pred_gbt)\n",
    "gbt_recall = recall_score(y_test, y_pred_gbt)\n",
    "gbt_f1 = f1_score(y_test, y_pred_gbt)\n",
    "\n",
    "\n",
    "metric_dict['Gradient Boosting'] = {'Accuracy': gbt_accuracy,\n",
    "                                                  'Precision': gbt_precision,\n",
    "                                                  'Recall': gbt_recall,\n",
    "                                                  'F1 Score': gbt_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T04:55:28.655558Z",
     "start_time": "2020-12-17T04:55:28.638706Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7802035152636447\n",
      "Precision: 0.8347862183478622\n",
      "Recall: 0.7179578721885042\n",
      "F1 Score: 0.7719769673704415\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Gradient Boosting did not perform better than other ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:36:55.848661Z",
     "start_time": "2020-12-17T15:36:55.836980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GridSearch SVC</th>\n",
       "      <td>0.826827</td>\n",
       "      <td>0.867271</td>\n",
       "      <td>0.786148</td>\n",
       "      <td>0.824719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting</th>\n",
       "      <td>0.823497</td>\n",
       "      <td>0.860320</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.822148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Logistic Regression</th>\n",
       "      <td>0.818316</td>\n",
       "      <td>0.851566</td>\n",
       "      <td>0.786505</td>\n",
       "      <td>0.817743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - Second Model</th>\n",
       "      <td>0.809436</td>\n",
       "      <td>0.847937</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.807333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest With Hypertuning</th>\n",
       "      <td>0.809436</td>\n",
       "      <td>0.847937</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.807333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch Random Forest - First Model</th>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.846275</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.806578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.780204</td>\n",
       "      <td>0.834786</td>\n",
       "      <td>0.717958</td>\n",
       "      <td>0.771977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.769658</td>\n",
       "      <td>0.823897</td>\n",
       "      <td>0.706533</td>\n",
       "      <td>0.760715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GridSearch ADABoost</th>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>0.727597</td>\n",
       "      <td>0.737204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADABoost and Random Forest</th>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>0.727597</td>\n",
       "      <td>0.737204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Accuracy  Precision    Recall  \\\n",
       "GridSearch SVC                             0.826827   0.867271  0.786148   \n",
       "Voting                                     0.823497   0.860320  0.787219   \n",
       "GridSearch Logistic Regression             0.818316   0.851566  0.786505   \n",
       "GridSearch Random Forest - Second Model    0.809436   0.847937  0.770439   \n",
       "GridSearch Random Forest With Hypertuning  0.809436   0.847937  0.770439   \n",
       "GridSearch Random Forest - First Model     0.808511   0.846275  0.770439   \n",
       "Gradient Boosting                          0.780204   0.834786  0.717958   \n",
       "Bagging                                    0.769658   0.823897  0.706533   \n",
       "GridSearch ADABoost                        0.731175   0.747067  0.727597   \n",
       "ADABoost and Random Forest                 0.731175   0.747067  0.727597   \n",
       "\n",
       "                                           F1 Score  \n",
       "GridSearch SVC                             0.824719  \n",
       "Voting                                     0.822148  \n",
       "GridSearch Logistic Regression             0.817743  \n",
       "GridSearch Random Forest - Second Model    0.807333  \n",
       "GridSearch Random Forest With Hypertuning  0.807333  \n",
       "GridSearch Random Forest - First Model     0.806578  \n",
       "Gradient Boosting                          0.771977  \n",
       "Bagging                                    0.760715  \n",
       "GridSearch ADABoost                        0.737204  \n",
       "ADABoost and Random Forest                 0.737204  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame metrics for evaluation\n",
    "df_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the ensemble models had some interesting results. Most of them did not perform any better than the vanilla models. I was surprised with the results for ADABoost. Since ADABoost first trains a base classifier and uses it to make predictions on the training set, I believe the results were going to be better.\n",
    "\n",
    "Other surprise was the GridSearch Random Forest poor performance. It performed worst than the Vanilla models. The winner was GridSearch SVC, which makes sense, since it was the best performing Vanilla model.\n",
    "\n",
    "GridSearch Logistic Regression also performed well. Proving to us that Logistic Regression cannot be underestimated in any project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
