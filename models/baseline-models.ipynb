{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will find the baseline models, also known as vanilla models. For the baseline models, I will run Logistic Regression, which is a basic but reliable model - it works well with binary classification?; Random Forest because I believe a Decision Tree could bring me good results but since Random Forest is a collection of Decision Trees, I can skip it and start with Random Forest; Naive Bayes, which is know for giving good results when applied to NLP; and Support Vector Machine, which is also known for working well with Natural Language Processing.\n",
    "\n",
    "I will try the vanilla models with the datasets vectorized with CountVectorizer, TF-IDF. I will try these models with and without lemmatization. I will also iterate the best models with a train set using SMOTE. I have fixed the class imbalance manually in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/preprossessing/data-cleaning.ipynb\">Data Cleaning</a> notebook. However, I'm curious to see if the models could have any improvement with SMOTE. I will use the `Spell_Checked` feature, since it's the cleanest one. I will not include other features from the original data set because the main objective is train a model using the reviews only.\n",
    "\n",
    "I have a binary classification, where the target will be 0 for negative review and 1 for positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprossess the dataset creating train and test datasets\n",
    "- Run models for each vectorizer used\n",
    "- Find the best models for each category\n",
    "- Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will import important packages and the data set we will use, which was already cleaned in the Data Cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:27.733573Z",
     "start_time": "2020-12-24T19:02:25.074263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP Packages\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Pandas Settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Solve warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Import pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:27.748350Z",
     "start_time": "2020-12-24T19:02:27.735598Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions import evaluation\n",
    "from functions import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the main dataset and the lemmatized X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:27.987314Z",
     "start_time": "2020-12-24T19:02:27.754018Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing cleaned dataset as a DataFrame\n",
    "df = pd.read_csv('../csv/Hotel_Review_Spell_Checked.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.018212Z",
     "start_time": "2020-12-24T19:02:27.989858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Positive_Review</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Reviews_Clean</th>\n",
       "      <th>Score</th>\n",
       "      <th>Spell_Checked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185010</td>\n",
       "      <td>St James Court A Taj Hotel London</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>the location was perfect</td>\n",
       "      <td>9.6</td>\n",
       "      <td>no negative the location was perfect</td>\n",
       "      <td>1</td>\n",
       "      <td>no negative the location was perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424531</td>\n",
       "      <td>H10 Metropolitan 4 Sup</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Everything was top notch staff were impeccable</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nothing  everything was top notch staff were ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nothing  everything was top notch staff were ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                         Hotel_Name Negative_Review  \\\n",
       "0        185010  St James Court A Taj Hotel London     No Negative   \n",
       "1        424531             H10 Metropolitan 4 Sup        Nothing    \n",
       "\n",
       "                                    Positive_Review  Reviewer_Score  \\\n",
       "0                         the location was perfect              9.6   \n",
       "1   Everything was top notch staff were impeccable             10.0   \n",
       "\n",
       "                                       Reviews_Clean  Score  \\\n",
       "0              no negative the location was perfect       1   \n",
       "1   nothing  everything was top notch staff were ...      1   \n",
       "\n",
       "                                       Spell_Checked  \n",
       "0              no negative the location was perfect   \n",
       "1   nothing  everything was top notch staff were ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking DataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Lemmatized X and Y Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lemmatized the feature variable `Spell_Checked` in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/preprossessing/data-cleaning.ipynb\">Data Cleaning</a> notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.043902Z",
     "start_time": "2020-12-24T19:02:28.020280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing lemmatized X and y variable\n",
    "X_lem = pickle.load(open('../pickle/X_lem.pkl', 'rb'))\n",
    "y_lem = pd.read_pickle('../pickle/y_lem.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.051641Z",
     "start_time": "2020-12-24T19:02:28.046058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.070112Z",
     "start_time": "2020-12-24T19:02:28.053881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping null values, if any\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, I will use the column `Spell_Checked` to create the features and `Score` as my target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.078404Z",
     "start_time": "2020-12-24T19:02:28.075236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating an X variable and y for my target\n",
    "X = df.Spell_Checked\n",
    "y = df.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.090496Z",
     "start_time": "2020-12-24T19:02:28.082087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset in train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.105312Z",
     "start_time": "2020-12-24T19:02:28.092772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the lemmatized dataset in train set and test set\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:28.804793Z",
     "start_time": "2020-12-24T19:02:28.107600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fitting into the train and test set\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:14:40.300696Z",
     "start_time": "2020-12-24T19:14:39.525412Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fitting into the train and test set\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:14:48.174707Z",
     "start_time": "2020-12-24T19:14:40.388449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert TF-IDF Vector back to a dataframe so I can get feature importance later\n",
    "X_train_tfidf_sparse = pd.DataFrame.sparse.from_spmatrix(\n",
    "    X_train_tfidf, columns=tfidf.get_feature_names())\n",
    "X_test_tfidf_sparse = pd.DataFrame.sparse.from_spmatrix(\n",
    "    X_test_tfidf, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:14:49.710452Z",
     "start_time": "2020-12-24T19:14:49.707589Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = X_train_tfidf_sparse\n",
    "X_test_tfidf = X_test_tfidf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-DF With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:37.689871Z",
     "start_time": "2020-12-24T19:02:37.103930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting into the lemmatized train and test set\n",
    "X_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "X_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My main focus is the accuracy metric. Have an accurate is important to be accurate. However, although fixing False Negatives is not crucial, I will also take a look at Recall and F1-Score to understand how my model is working. Since it is not my main focus, I will not mentioned in the individual analysis on my models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will run the Logistic Regression, Random Forest, Naive Bayes, and SVC models for the dataset that was vectorized using Count Vectorizer. Then, I will pick the three best models and move to the next section, which will be using TF-IDF.\n",
    "\n",
    "I will also create dictionaries for each model so that I will be able to create a DataFrame with the models' results. For better visualization, I will evaluate each model by the end of each section.\n",
    "\n",
    "\n",
    "<b>Note:</b> You will see that I will instantiate the same model multiple times. This will be done so that you can run each model individually, without having the run all the cells for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling With Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will run the Logistic Regression, Random Forest, Naive Bayes, and SVC models for the dataset that was vectorized using Count Vectorizer. Then, I will pick the three best models and move to the next section, which will be using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:38.522166Z",
     "start_time": "2020-12-24T19:02:37.691787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lg_cv = LogisticRegression()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lg_cv.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lg_cv = lg_cv.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:38.545084Z",
     "start_time": "2020-12-24T19:02:38.524193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lr_cv_accuracy = accuracy_score(y_test, y_lg_cv)\n",
    "lr_cv_precision = precision_score(y_test, y_lg_cv)\n",
    "lr_cv_recall = recall_score(y_test, y_lg_cv)\n",
    "lr_cv_f1 = f1_score(y_test, y_lg_cv)\n",
    "\n",
    "metric_dict = {}\n",
    "metric_dict['Vanilla Logisitic Regression CV'] = {'Accuracy': lr_cv_accuracy,\n",
    "                                                'Precision': lr_cv_precision,\n",
    "                                                'Recall': lr_cv_recall,\n",
    "                                                'F1 Score': lr_cv_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:40.537202Z",
     "start_time": "2020-12-24T19:02:38.547129Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.798149861239593\n",
      "Precision: 0.8104575163398693\n",
      "Recall: 0.7968582649053909\n",
      "F1 Score: 0.8036003600360035\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.80758557 0.80049337 0.80080173 0.80727721 0.81431215]\n",
      "Min:  0.800493\n",
      "Max:  0.814312\n",
      "Mean:  0.806094\n",
      "Range:  0.013819\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression baseline evaluation\n",
    "evaluation(y_test, y_lg_cv)\n",
    "cross_validation(lg_cv, X_train_cv, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:40.552341Z",
     "start_time": "2020-12-24T19:02:40.539536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vanilla Logisitic Regression CV</th>\n",
       "      <td>0.79815</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.8036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Accuracy  Precision    Recall  F1 Score\n",
       "Vanilla Logisitic Regression CV   0.79815   0.810458  0.796858    0.8036"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:41.444025Z",
     "start_time": "2020-12-24T19:02:40.555074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a confusion matrix\n",
    "plt.figure(figsize=(6, 7))\n",
    "mat = confusion_matrix(y_test, y_lg_cv)\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'], cmap=\"Blues\")\n",
    "plt.xlabel('True Label', fontsize= 18)\n",
    "plt.ylabel('Predicted Label', fontsize= 18)\n",
    "plt.title('Baseline Logistic Regression Confusion Matrix', fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "b, t = plt.ylim() \n",
    "t -= 0.05 \n",
    "plt.ylim(b, t) \n",
    "plt.savefig('../images/confusion-matrix-baseline-lg-model.png', bbox_inches = \"tight\", pad_inches=.5, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, Logistic Regression performed quite well with an accuracy of 0.7981. My main metric is accuracy and it seems that Logistic Regression is able to correctly understand the positive and negative reviews. Looking at the cross validations, it seems that the train set performs similar to the test set with a small difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:47.661383Z",
     "start_time": "2020-12-24T19:02:41.446527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_baseline = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_cv = rf_baseline.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:02:47.682762Z",
     "start_time": "2020-12-24T19:02:47.663238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_cv_accuracy = accuracy_score(y_test, y_rf_cv)\n",
    "rf_cv_precision = precision_score(y_test, y_rf_cv)\n",
    "rf_cv_recall = recall_score(y_test, y_rf_cv)\n",
    "rf_cv_f1 = f1_score(y_test, y_rf_cv)\n",
    "\n",
    "metric_dict['Vanilla Random Forest CV'] = {'Accuracy': rf_cv_accuracy,\n",
    "                                                'Precision': rf_cv_precision,\n",
    "                                                'Recall': rf_cv_recall,\n",
    "                                                'F1 Score': rf_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:03:21.954014Z",
     "start_time": "2020-12-24T19:02:47.690765Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.798149861239593\n",
      "Precision: 0.8104575163398693\n",
      "Recall: 0.7968582649053909\n",
      "F1 Score: 0.8036003600360035\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.80635214 0.80696886 0.79617638 0.81282763 0.81647131]\n",
      "Min:  0.796176\n",
      "Max:  0.816471\n",
      "Mean:  0.807759\n",
      "Range:  0.020295\n"
     ]
    }
   ],
   "source": [
    "# Random Forest baseline evaluation\n",
    "evaluation(y_test, y_lg_cv)\n",
    "cross_validation(rf_baseline, X_train_cv, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was able to do slightly better than Logistic Regression with an accuracy of 0.8031. Let's see if other models are able to perform better than that. Similarly to Logistic Regression, when looking at cross validation, we can see that the model performed similarly in train set and test set with no signs of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:03:21.971744Z",
     "start_time": "2020-12-24T19:03:21.959086Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Naive Bayes Model\n",
    "nb_base_cv = MultinomialNB(alpha = .01)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "nb_base_cv.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_nb_base_cv = nb_base_cv.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:03:21.991825Z",
     "start_time": "2020-12-24T19:03:21.973957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "nb_cv_accuracy = accuracy_score(y_test, y_nb_base_cv)\n",
    "nb_cv_precision = precision_score(y_test, y_nb_base_cv)\n",
    "nb_cv_recall = recall_score(y_test, y_nb_base_cv)\n",
    "nb_cv_f1 = f1_score(y_test, y_nb_base_cv)\n",
    "\n",
    "metric_dict['Vanilla Naive Bayes CV'] = {'Accuracy': nb_cv_accuracy,\n",
    "                                                'Precision': nb_cv_precision,\n",
    "                                                'Recall': nb_cv_recall,\n",
    "                                                'F1 Score': nb_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:03:22.057279Z",
     "start_time": "2020-12-24T19:03:21.994335Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7811285846438483\n",
      "Precision: 0.7935413642960812\n",
      "Recall: 0.7807925740806855\n",
      "F1 Score: 0.7871153500089977\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.78939254 0.79925994 0.77582485 0.79401788 0.80289944]\n",
      "Min:  0.775825\n",
      "Max:  0.802899\n",
      "Mean:  0.792279\n",
      "Range:  0.027075\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes baseline evaluation\n",
    "evaluation(y_test, y_nb_base_cv)\n",
    "cross_validation(nb_base_cv, X_train_cv, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes didn't perform as well as Logistic Regression and Random Forest. It's a little surprising to me, since it's well known for being one of the best models for NLP. When looking at cross-validation, we can see that the model performed similarly in the train set compared to the test set with a range between the minimum and maximum a little higher than Logistic Regression and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:04:45.466507Z",
     "start_time": "2020-12-24T19:03:22.060084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Naive Bayes Model\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "svc.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_pred_svc = svc.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:04:45.490184Z",
     "start_time": "2020-12-24T19:04:45.468747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_cv_accuracy = accuracy_score(y_test, y_pred_svc)\n",
    "svc_cv_precision = precision_score(y_test, y_pred_svc)\n",
    "svc_cv_recall = recall_score(y_test, y_pred_svc)\n",
    "svc_cv_f1 = f1_score(y_test, y_pred_svc)\n",
    "\n",
    "metric_dict['Vanilla SVC CV'] = {'Accuracy': svc_cv_accuracy,\n",
    "                                  'Precision': svc_cv_precision,\n",
    "                                  'Recall': svc_cv_recall,\n",
    "                                  'F1 Score': svc_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:07:58.754583Z",
     "start_time": "2020-12-24T19:04:45.493167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7839037927844589\n",
      "Precision: 0.7942342342342342\n",
      "Recall: 0.7868618350589075\n",
      "F1 Score: 0.7905308464849354\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.78353377 0.79432624 0.78939254 0.7875424  0.80043183]\n",
      "Min:  0.783534\n",
      "Max:  0.800432\n",
      "Mean:  0.791045\n",
      "Range:  0.016898\n"
     ]
    }
   ],
   "source": [
    "# SVC baseline evaluation\n",
    "evaluation(y_test, y_pred_svc)\n",
    "cross_validation(svc, X_train_cv, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC also didn't perform as well as Logistic Regression and Random Forest. However, it performed better than Naive Bayes. Thus, I believe it has potential to perform better in further iterations. Using cross-validation I was able to see that the model did not overfit in the trai set and performed similarly to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:07:58.774152Z",
     "start_time": "2020-12-24T19:07:58.757609Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vanilla Random Forest CV</th>\n",
       "      <td>0.803330</td>\n",
       "      <td>0.838659</td>\n",
       "      <td>0.768297</td>\n",
       "      <td>0.801938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Accuracy  Precision    Recall  F1 Score\n",
       "Vanilla Random Forest CV         0.803330   0.838659  0.768297  0.801938\n",
       "Vanilla Logisitic Regression CV  0.798150   0.810458  0.796858  0.803600\n",
       "Vanilla SVC CV                   0.783904   0.794234  0.786862  0.790531\n",
       "Vanilla Naive Bayes CV           0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the models performed quite well for baseline models. The three best models were Random Forest, Logistic Regression, and SVC. Naive Bayes did a good job as well, but it under performed if compared to the other models in the accuracy metric, which is my focus for this project.\n",
    "\n",
    "It is interesting to see that Logistic Regression performed in second place between these baseline models. It tells us that although not usually seen as a powerful model, it holds it's surprises.\n",
    "\n",
    "I run every model with cross-validation with 5 folds and I don't see any signs of overfit or underfit with the train set. All the models performed similarly in the train set as well in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will see how each of the three best models in the previous section performs when using the TF-IDF vectorizer. Too keep each to read, the evaluations will be at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:08:01.974205Z",
     "start_time": "2020-12-24T19:07:58.777688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lg_tfidf = LogisticRegression()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lg_tfidf.fit(X_train_tfidf, y_train) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lg_base_tfidf = lg_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:08:01.997684Z",
     "start_time": "2020-12-24T19:08:01.976462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lg_tfidf_accuracy = accuracy_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_precision = precision_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_recall = recall_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_f1 = f1_score(y_test, y_lg_base_tfidf)\n",
    "\n",
    "metric_dict['Vanila Log Reg TF-IDF'] = {'Accuracy': lg_tfidf_accuracy,\n",
    "                                                'Precision': lg_tfidf_precision,\n",
    "                                                'Recall': lg_tfidf_recall,\n",
    "                                                'F1 Score': lg_tfidf_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:08:49.765560Z",
     "start_time": "2020-12-24T19:08:01.999933Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8170212765957446\n",
      "Precision: 0.8481936971560338\n",
      "Recall: 0.7879328811138879\n",
      "F1 Score: 0.8169535443272257\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.81776133 0.82269504 0.81591119 0.82146161 0.82973473]\n",
      "Min:  0.815911\n",
      "Max:  0.829735\n",
      "Mean:  0.821513\n",
      "Range:  0.013824\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test, y_lg_base_tfidf)\n",
    "cross_validation(lg_tfidf, X_train_tfidf, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement with Logistic Regressing and TF-IDF. The accuracy improved from 0.7981 to 0.8170. We can see that the model performed similarly in every fold when we use cross-validation. As the previous model, there is no underfitting or overfitting in the train set and it performs very similarly to test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:09:10.529877Z",
     "start_time": "2020-12-24T19:08:49.768405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "rf_baseline = RandomForestClassifier()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_base_tfidf = rf_baseline.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:09:10.547757Z",
     "start_time": "2020-12-24T19:09:10.531952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_tfidf_accuracy = accuracy_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_precision = precision_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_recall = recall_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_f1 = f1_score(y_test, y_rf_base_tfidf)\n",
    "\n",
    "metric_dict['Vanilla Random Forest TF-IDF'] = {'Accuracy': rf_tfidf_accuracy,\n",
    "                                                'Precision': rf_tfidf_precision,\n",
    "                                                'Recall': rf_tfidf_recall,\n",
    "                                                'F1 Score': rf_tfidf_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:49.298852Z",
     "start_time": "2020-12-24T19:09:10.549918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8061054579093432\n",
      "Precision: 0.8485089463220676\n",
      "Recall: 0.7618707604426991\n",
      "F1 Score: 0.8028592927012792\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.79648474 0.80357693 0.79802652 0.8115942  0.81061073]\n",
      "Min:  0.796485\n",
      "Max:  0.811594\n",
      "Mean:  0.804059\n",
      "Range:  0.015109\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test, y_rf_base_tfidf)\n",
    "cross_validation(rf_baseline, X_train_tfidf, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with TF-IDF also performed slightly better than when I tried it with Count Vectorizer. For now, it seems that TF-IDF works better as a vectorizer for this dataset. Looking at cross-validation, the train set performed similarly with the test set as it happened to previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:15:41.667828Z",
     "start_time": "2020-12-24T19:15:41.654130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vanila Log Reg TF-IDF</th>\n",
       "      <td>0.817021</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Random Forest TF-IDF</th>\n",
       "      <td>0.806105</td>\n",
       "      <td>0.848509</td>\n",
       "      <td>0.761871</td>\n",
       "      <td>0.802859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Random Forest CV</th>\n",
       "      <td>0.803330</td>\n",
       "      <td>0.838659</td>\n",
       "      <td>0.768297</td>\n",
       "      <td>0.801938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanilla Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Accuracy  Precision    Recall  F1 Score\n",
       "Vanila Log Reg TF-IDF            0.817021   0.848194  0.787933  0.816954\n",
       "Vanilla Random Forest TF-IDF     0.806105   0.848509  0.761871  0.802859\n",
       "Vanilla Random Forest CV         0.803330   0.838659  0.768297  0.801938\n",
       "Vanilla Logisitic Regression CV  0.798150   0.810458  0.796858  0.803600\n",
       "Vanilla SVC CV                   0.783904   0.794234  0.786862  0.790531\n",
       "Vanilla Naive Bayes CV           0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.to_csv('../csv/baseline-models_tfidf.csv')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all the models using TF-IDF had a better performance than the models using CountVectorizer. SVC model had an big improvement compared to other models and Logistic Regression was able to perform better than Random Forest, which is surprising. The model that had the lowest improvement was Random Forest, which we can see that it changed very little.\n",
    "\n",
    "I will now test these three models with the lemmatized dataset and see if there is any improvement.\n",
    "\n",
    "I used cross-validation in the train set and none of the iterations had underfitting or overfitting. All the models had similar performance in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF With Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try the three best models using the lemmatized variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:15:59.873060Z",
     "start_time": "2020-12-24T19:15:59.454392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lr_lem = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lr_lem.fit(X_train_lem, y_train_lem) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lr_base_tfidf_lem = lr_lem.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:16:01.670493Z",
     "start_time": "2020-12-24T19:16:01.653957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lr_lem_accuracy = accuracy_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_precision = precision_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_recall = recall_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_f1 = f1_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "\n",
    "metric_dict['Vanilla Log Reg Lem'] = {'Accuracy': lr_lem_accuracy,\n",
    "                                                'Precision': lr_lem_precision,\n",
    "                                                'Recall': lr_lem_recall,\n",
    "                                                'F1 Score': lr_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:16:04.466980Z",
     "start_time": "2020-12-24T19:16:03.487459Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8177613320999075\n",
      "Precision: 0.8314990512333966\n",
      "Recall: 0.8019765739385066\n",
      "F1 Score: 0.8164710266443078\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.82485353 0.82732038 0.82331175 0.80511872 0.8127699 ]\n",
      "Min:  0.805119\n",
      "Max:  0.82732\n",
      "Mean:  0.818675\n",
      "Range:  0.022202\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_lr_base_tfidf_lem)\n",
    "cross_validation(lr_lem, X_train_lem, y_train_lem, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration performed slightly better than the iteration without lemmatization: 0.81777 vs 0.81770. It's a very insignificant improvement. However, we can see that the F1 Score decreased when compared to the model without lemmatization, which is the second metric that I am considering. Looking at cross-validation, both models performed well. However, the model without lemmatization has a lower range in different folds in the train set. Although very small. For now, I consider the model without lemmatization a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:16:29.703455Z",
     "start_time": "2020-12-24T19:16:07.852317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_lem = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_lem.fit(X_train_lem, y_train_lem) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_base_lem = rf_lem.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:16:29.721699Z",
     "start_time": "2020-12-24T19:16:29.705427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_lem_accuracy = accuracy_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_precision = precision_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_recall = recall_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_f1 = f1_score(y_test_lem, y_rf_base_lem)\n",
    "\n",
    "metric_dict['Vanilla Random Forest Lem'] = {'Accuracy': rf_lem_accuracy,\n",
    "                                                'Precision': rf_lem_precision,\n",
    "                                                'Recall': rf_lem_recall,\n",
    "                                                'F1 Score': rf_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:18:14.946200Z",
     "start_time": "2020-12-24T19:16:29.724612Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7988899167437558\n",
      "Precision: 0.8196657598134474\n",
      "Recall: 0.7719619326500732\n",
      "F1 Score: 0.7950989632422243\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.80881899 0.80296022 0.80296022 0.80357693 0.80074028]\n",
      "Min:  0.80074\n",
      "Max:  0.808819\n",
      "Mean:  0.803811\n",
      "Range:  0.008079\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_rf_base_lem)\n",
    "cross_validation(rf_lem, X_train_lem, y_train_lem, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with lemmatization did not perform any better than the model without lemmatization. However, it's important to noticed that it had the lowest range of difference between the 5 folds in cross-validation. All sets performed very similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:19:58.222650Z",
     "start_time": "2020-12-24T19:18:14.948749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline SVC Model\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "svc.fit(X_train_lem, y_train_lem)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_pred_svc_lem = svc.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:19:58.277895Z",
     "start_time": "2020-12-24T19:19:58.229143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_lem_accuracy = accuracy_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_precision = precision_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_recall = recall_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_f1 = f1_score(y_test_lem, y_pred_svc_lem)\n",
    "\n",
    "metric_dict['Vanilla Random Forest Lem'] = {'Accuracy': svc_lem_accuracy,\n",
    "                                                'Precision': svc_lem_precision,\n",
    "                                                'Recall': svc_lem_recall,\n",
    "                                                'F1 Score': svc_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:24:06.987735Z",
     "start_time": "2020-12-24T19:19:58.288721Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8231267345050879\n",
      "Precision: 0.8493312352478364\n",
      "Recall: 0.7902635431918009\n",
      "F1 Score: 0.8187334091770953\n",
      "\n",
      "Cross-Validation Accuracy Scores: [0.82115325 0.8279371  0.82392846 0.81221092 0.82202344]\n",
      "Min:  0.812211\n",
      "Max:  0.827937\n",
      "Mean:  0.821451\n",
      "Range:  0.015726\n"
     ]
    }
   ],
   "source": [
    "# SCV TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_pred_svc_lem)\n",
    "cross_validation(svc, X_train_lem, y_train_lem, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.279851Z",
     "start_time": "2020-12-24T19:02:25.317Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Random Forest is our best model so far compared to all the previous model. However, the baseline SVC model using TF-IDF is very close to it with a higher precision and higher F1 Score. They aren't my main focus when it comes to metrics, but I might take in consideration when choosing the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fixed the class imbalance manually. However, I'm curious to see how my best models will behave using SMOTE. For this reason I'll run two of my best models and see how it would perform with SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.281678Z",
     "start_time": "2020-12-24T19:02:25.323Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train_smote_lem, y_train_smote_lem = smote.fit_sample(X_train_lem, y_train_lem) \n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_smote_tfidf, y_train_smote_tfidf = smote.fit_sample(X_train_tfidf, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.283382Z",
     "start_time": "2020-12-24T19:02:25.327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_bas eline = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_smote_lem, y_train_smote_lem)\n",
    "\n",
    "# Predicting the model in the X variable of the test set### Random Forest\n",
    "y_rf_base_lem_tfidf_smote = rf_baseline.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.284923Z",
     "start_time": "2020-12-24T19:02:25.332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_smote_accuracy = accuracy_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_precision = precision_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_recall = recall_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_f1 = f1_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "\n",
    "metric_dict['Vanilla Random Forest SMOTE'] = {'Accuracy': rf_smote_accuracy,\n",
    "                                                'Precision': rf_smote_precision,\n",
    "                                                'Recall': rf_smote_recall,\n",
    "                                                'F1 Score': rf_smote_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.286259Z",
     "start_time": "2020-12-24T19:02:25.336Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "cross_validation(rf_baseline, X_train_smote_lem, y_train_smote_lem, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest did not perform any better using SMOTE. Since the class imbalance was manually fixed, I was not expecting much difference. The model performed similarly as in previous iterations and id doesn't see to have overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T19:40:32.733033Z",
     "start_time": "2020-12-16T19:40:32.730826Z"
    }
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.287841Z",
     "start_time": "2020-12-24T19:02:25.341Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_smote_tfidf, y_train_smote_tfidf)\n",
    "y_pred_svc_smote = svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.289224Z",
     "start_time": "2020-12-24T19:02:25.345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_smote_accuracy = accuracy_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_precision = precision_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_recall = recall_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_f1 = f1_score(y_test, y_pred_svc_smote)\n",
    "\n",
    "metric_dict['Vanilla SVC SMOTE'] = {'Accuracy': svc_smote_accuracy,\n",
    "                                                'Precision': svc_smote_precision,\n",
    "                                                'Recall': svc_smote_recall,\n",
    "                                                'F1 Score': svc_smote_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.290808Z",
     "start_time": "2020-12-24T19:02:25.350Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation(y_test, y_pred_svc_smote)\n",
    "cross_validation(svc, X_train_smote_lem, y_train_smote_lem, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check complete evaluation of all the models below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.292638Z",
     "start_time": "2020-12-24T19:02:25.354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the SMOTE was able to improve the accuracy metric by a very small different. However, Recall and F1 Score dropped, which tells me that the model is not as good as the version without SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T19:47:12.077415Z",
     "start_time": "2020-12-16T19:47:12.075016Z"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.294545Z",
     "start_time": "2020-12-24T19:02:25.359Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.to_csv('../csv/baseline-models_evaluations.csv')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running multiple baseline models, we were able to verify a few points:\n",
    "- Our best model was a Random Forest using lemmatized words. However, the F1 score was lower than other models. Since I will focus on F1 Score further in this project, the winner was SVC using TF-IDF. \n",
    "- Random Forest, CVC, and Logistic Regression are the best models to use in NLP in these cases\n",
    "- TF-IDF was able improve all all our models.\n",
    "- Naive Bayes was our worst model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.295940Z",
     "start_time": "2020-12-24T19:02:25.363Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(16,6))\n",
    "evaluation_df['Accuracy'].sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=75, fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.297596Z",
     "start_time": "2020-12-24T19:02:25.368Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting a confusion matrix\n",
    "plt.figure(figsize=(6, 7))\n",
    "mat = confusion_matrix(y_test, y_pred_svc_tfidf)\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'], cmap=\"Blues\")\n",
    "plt.xlabel('True Label', fontsize= 18)\n",
    "plt.ylabel('Predicted Label', fontsize= 18)\n",
    "plt.title('Baseline SVC TF-IDF Confusion Matrix', fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "b, t = plt.ylim() \n",
    "t -= 0.05 \n",
    "plt.ylim(b, t) \n",
    "plt.savefig('../images/confusion-matrix-baseline-model.png', bbox_inches = \"tight\", pad_inches=.5, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that my model was able to predict correctly 84.47% of the positive reviews and 72.10% of the negative reviews. This means that I need to improve my model's ability to better predict negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with my best models in mind, I will run a few ensemble models in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/models/ensemble-models.ipynb\">Data Cleaning</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.299110Z",
     "start_time": "2020-12-24T19:02:25.373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/lg_tfidf.pkl\",'wb')\n",
    "pickle.dump(lg_tfidf, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.300841Z",
     "start_time": "2020-12-24T19:02:25.377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/vanilla_model_evaluation.pkl\",'wb')\n",
    "pickle.dump(evaluation_df, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.302215Z",
     "start_time": "2020-12-24T19:02:25.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/X_train_tfidf.pkl\",'wb')\n",
    "pickle.dump(X_train_tfidf, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.303614Z",
     "start_time": "2020-12-24T19:02:25.384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Test Set - Features\n",
    "pickle_out = open(\"../pickle/X_test_tfidf.pkl\",'wb')\n",
    "pickle.dump(X_test_tfidf, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.305154Z",
     "start_time": "2020-12-24T19:02:25.388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train and Test Set  Target\n",
    "y_train.to_pickle(\"../pickle/y_train.pkl\")\n",
    "y_test.to_pickle(\"../pickle/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.306569Z",
     "start_time": "2020-12-24T19:02:25.392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/X_train_cv.pkl\",'wb')\n",
    "pickle.dump(X_train_cv, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.308318Z",
     "start_time": "2020-12-24T19:02:25.396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Test Set - Features\n",
    "pickle_out = open(\"../pickle/X_test_cv.pkl\",'wb')\n",
    "pickle.dump(X_test_cv, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.310083Z",
     "start_time": "2020-12-24T19:02:25.400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/rf_lem_model.pkl\",'wb')\n",
    "pickle.dump(rf_lem, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.311979Z",
     "start_time": "2020-12-24T19:02:25.404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/X_train_lem.pkl\",'wb')\n",
    "pickle.dump(X_train_lem, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.313498Z",
     "start_time": "2020-12-24T19:02:25.408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Test Set - Features\n",
    "pickle_out = open(\"../pickle/X_test_lem.pkl\",'wb')\n",
    "pickle.dump(X_test_lem, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.315002Z",
     "start_time": "2020-12-24T19:02:25.412Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train and Test Set - Target\n",
    "y_train_lem.to_pickle(\"../pickle/y_train_lem.pkl\")\n",
    "y_test_lem.to_pickle(\"../pickle/y_test_lem.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T19:11:50.316505Z",
     "start_time": "2020-12-24T19:02:25.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickle best model using CountVectorizer\n",
    "pickle_out = open(\"../pickle/lg_cv.pickle\",\"wb\")\n",
    "pickle.dump(lg_cv, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
