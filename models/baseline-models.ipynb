{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will find the baseline models, also known as vanilla models. For the baseline models, I will run Logistic Regression, which is a basic but reliable model - it works well with binary classification?; Random Forest because I believe a Decision Tree could bring me good results but since Random Forest is a collection of Decision Trees, I can skip it and start with Random Forest; Naive Bayes, which is know for giving good results when applied to NLP; and Support Vector Machine, which is also known for working well with Natural Language Processing.\n",
    "\n",
    "I will try the vanilla models with the datasets vectorized with CountVectorizer, TF-IDF. I will try these models with and without lemmatization. I will also iterate the best models with a train set using SMOTE. I have fixed the class imbalance manually in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/preprossessing/data-cleaning.ipynb\">Data Cleaning</a> notebook. However, I'm curious to see if the models could have any improvement with SMOTE. I will use the `Spell_Checked` feature, since it's the cleanest one. I will not include other features from the original data set because the main objective is train a model using the reviews only.\n",
    "\n",
    "I have a binary classification, where the target will be 0 for negative review and 1 for positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprossess the dataset creating train and test datasets\n",
    "- Run models for each vectorizer used\n",
    "- Find the best models for each category\n",
    "- Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will import important packages and the data set we will use, which was already cleaned in the Data Cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:05.828977Z",
     "start_time": "2020-12-17T17:14:03.143380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# NLP Packages\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Pandas Settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Solve warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Import pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the main dataset and the lemmatized X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.056562Z",
     "start_time": "2020-12-17T17:14:05.831395Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing cleaned dataset as a DataFrame\n",
    "df = pd.read_csv('../csv/Hotel_Review_Spell_Checked.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.103613Z",
     "start_time": "2020-12-17T17:14:06.065612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Positive_Review</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Reviews_Clean</th>\n",
       "      <th>Score</th>\n",
       "      <th>Spell_Checked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185010</td>\n",
       "      <td>St James Court A Taj Hotel London</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>the location was perfect</td>\n",
       "      <td>9.6</td>\n",
       "      <td>no negative the location was perfect</td>\n",
       "      <td>1</td>\n",
       "      <td>no negative the location was perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424531</td>\n",
       "      <td>H10 Metropolitan 4 Sup</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Everything was top notch staff were impeccable</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nothing  everything was top notch staff were ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nothing  everything was top notch staff were ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                         Hotel_Name Negative_Review  \\\n",
       "0        185010  St James Court A Taj Hotel London     No Negative   \n",
       "1        424531             H10 Metropolitan 4 Sup        Nothing    \n",
       "\n",
       "                                    Positive_Review  Reviewer_Score  \\\n",
       "0                         the location was perfect              9.6   \n",
       "1   Everything was top notch staff were impeccable             10.0   \n",
       "\n",
       "                                       Reviews_Clean  Score  \\\n",
       "0              no negative the location was perfect       1   \n",
       "1   nothing  everything was top notch staff were ...      1   \n",
       "\n",
       "                                       Spell_Checked  \n",
       "0              no negative the location was perfect   \n",
       "1   nothing  everything was top notch staff were ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking DataFrame\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Lemmatized X and Y Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lemmatized the feature variable `Spell_Checked` in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/preprossessing/data-cleaning.ipynb\">Data Cleaning</a> notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.134982Z",
     "start_time": "2020-12-17T17:14:06.107762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing lemmatized X and y variable\n",
    "X_lem = pickle.load(open('../pickle/X_lem.pkl', 'rb'))\n",
    "y_lem = pd.read_pickle('../pickle/y_lem.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.145326Z",
     "start_time": "2020-12-17T17:14:06.137359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.169793Z",
     "start_time": "2020-12-17T17:14:06.148047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping null values, if any\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, I will use the column `Spell_Checked` to create the features and `Score` as my target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.177298Z",
     "start_time": "2020-12-17T17:14:06.174554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating an X variable and y for my target\n",
    "X = df.Spell_Checked\n",
    "y = df.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.190927Z",
     "start_time": "2020-12-17T17:14:06.180978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset in train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:06.207104Z",
     "start_time": "2020-12-17T17:14:06.193382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the lemmatized dataset in train set and test set\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:07.127436Z",
     "start_time": "2020-12-17T17:14:06.209557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fitting into the train and test set\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:08.004567Z",
     "start_time": "2020-12-17T17:14:07.129481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fitting into the train and test set\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-DF With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:08.756759Z",
     "start_time": "2020-12-17T17:14:08.007213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting into the lemmatized train and test set\n",
    "X_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "X_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric below will simplify the model evaluation. My main focus is the accuracy metric. Have an accurate is important to be accurate. However, although fixing False Negatives is not crucial, I will also take a look at Recall and F1-Score to understand how my model is working. Since it is not my main focus, I will not mentioned in the individual analysis on my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:08.767290Z",
     "start_time": "2020-12-17T17:14:08.759387Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load ../functions/evaluation.py\n",
    "# Evaluation function\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "\n",
    "    print('Evaluation Metrics:')\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(metrics.precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(metrics.recall_score(y_true, y_pred)))\n",
    "    print('F1 Score: ' + str(metrics.f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will run the Logistic Regression, Random Forest, Naive Bayes, and SVC models for the dataset that was vectorized using Count Vectorizer. Then, I will pick the three best models and move to the next section, which will be using TF-IDF.\n",
    "\n",
    "I will also create dictionaries for each model so that I will be able to create a DataFrame with the models' results. For better visualization, I will evaluate each model by the end of each section.\n",
    "\n",
    "\n",
    "<b>Note:</b> You will see that I will instantiate the same model multiple times. This will be done so that you can run each model individually, without having the run all the cells for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling With Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will run the Logistic Regression, Random Forest, Naive Bayes, and SVC models for the dataset that was vectorized using Count Vectorizer. Then, I will pick the three best models and move to the next section, which will be using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:09.752212Z",
     "start_time": "2020-12-17T17:14:08.771740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lg_baseline = LogisticRegression()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lg_baseline.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lg_base_cv = lg_baseline.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:09.778704Z",
     "start_time": "2020-12-17T17:14:09.754588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lr_cv_accuracy = accuracy_score(y_test, y_lg_base_cv)\n",
    "lr_cv_precision = precision_score(y_test, y_lg_base_cv)\n",
    "lr_cv_recall = recall_score(y_test, y_lg_base_cv)\n",
    "lr_cv_f1 = f1_score(y_test, y_lg_base_cv)\n",
    "\n",
    "metric_dict = {}\n",
    "metric_dict['Baseline Logisitic Regression CV'] = {'Accuracy': lr_cv_accuracy,\n",
    "                                                'Precision': lr_cv_precision,\n",
    "                                                'Recall': lr_cv_recall,\n",
    "                                                'F1 Score': lr_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:09.803846Z",
     "start_time": "2020-12-17T17:14:09.782801Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.798149861239593\n",
      "Precision: 0.8104575163398693\n",
      "Recall: 0.7968582649053909\n",
      "F1 Score: 0.8036003600360035\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression baseline evaluation\n",
    "evaluation(y_test, y_lg_base_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:09.820689Z",
     "start_time": "2020-12-17T17:14:09.806872Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.79815</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.8036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accuracy  Precision    Recall  F1 Score\n",
       "Baseline Logisitic Regression CV   0.79815   0.810458  0.796858    0.8036"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, Logistic Regression performed quite well with an accuracy of 0.7981. My main metric is accuracy and it seems that Logistic Regression is able to correctly understand the positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.029392Z",
     "start_time": "2020-12-17T17:14:09.823023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_baseline = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_cv = rf_baseline.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.049759Z",
     "start_time": "2020-12-17T17:14:16.031602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_cv_accuracy = accuracy_score(y_test, y_rf_cv)\n",
    "rf_cv_precision = precision_score(y_test, y_rf_cv)\n",
    "rf_cv_recall = recall_score(y_test, y_rf_cv)\n",
    "rf_cv_f1 = f1_score(y_test, y_rf_cv)\n",
    "\n",
    "metric_dict['Baseline Random Forest CV'] = {'Accuracy': rf_cv_accuracy,\n",
    "                                                'Precision': rf_cv_precision,\n",
    "                                                'Recall': rf_cv_recall,\n",
    "                                                'F1 Score': rf_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.069970Z",
     "start_time": "2020-12-17T17:14:16.051892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8022201665124884\n",
      "Precision: 0.8377535101404057\n",
      "Recall: 0.7668689753659408\n",
      "F1 Score: 0.8007455731593663\n"
     ]
    }
   ],
   "source": [
    "# Random Forest baseline evaluation\n",
    "evaluation(y_test, y_rf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was able to do slightly better than Logistic Regression with an accuracy of 0.8031. Let's see if other models are able to perform better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.084390Z",
     "start_time": "2020-12-17T17:14:16.072162Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Naive Bayes Model\n",
    "nb_base_cv = MultinomialNB(alpha = .01)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "nb_base_cv.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_nb_base_cv = nb_base_cv.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.110378Z",
     "start_time": "2020-12-17T17:14:16.092339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "nb_cv_accuracy = accuracy_score(y_test, y_nb_base_cv)\n",
    "nb_cv_precision = precision_score(y_test, y_nb_base_cv)\n",
    "nb_cv_recall = recall_score(y_test, y_nb_base_cv)\n",
    "nb_cv_f1 = f1_score(y_test, y_nb_base_cv)\n",
    "\n",
    "metric_dict['Baseline Naive Bayes CV'] = {'Accuracy': nb_cv_accuracy,\n",
    "                                                'Precision': nb_cv_precision,\n",
    "                                                'Recall': nb_cv_recall,\n",
    "                                                'F1 Score': nb_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:14:16.132330Z",
     "start_time": "2020-12-17T17:14:16.115164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7811285846438483\n",
      "Precision: 0.7935413642960812\n",
      "Recall: 0.7807925740806855\n",
      "F1 Score: 0.7871153500089977\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes baseline evaluation\n",
    "evaluation(y_test, y_nb_base_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes didn't perform as well as Logistic Regression and Random Forest. It's a little surprising to me, since it's well known for being one of the best models for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.216937Z",
     "start_time": "2020-12-17T17:14:16.134557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Naive Bayes Model\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "svc.fit(X_train_cv, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_pred_svc = svc.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.234052Z",
     "start_time": "2020-12-17T17:15:36.218725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_cv_accuracy = accuracy_score(y_test, y_pred_svc)\n",
    "svc_cv_precision = precision_score(y_test, y_pred_svc)\n",
    "svc_cv_recall = recall_score(y_test, y_pred_svc)\n",
    "svc_cv_f1 = f1_score(y_test, y_pred_svc)\n",
    "\n",
    "metric_dict['Baseline SVC CV'] = {'Accuracy': svc_cv_accuracy,\n",
    "                                  'Precision': svc_cv_precision,\n",
    "                                  'Recall': svc_cv_recall,\n",
    "                                  'F1 Score': svc_cv_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.252326Z",
     "start_time": "2020-12-17T17:15:36.235979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7839037927844589\n",
      "Precision: 0.7942342342342342\n",
      "Recall: 0.7868618350589075\n",
      "F1 Score: 0.7905308464849354\n"
     ]
    }
   ],
   "source": [
    "# SVC baseline evaluation\n",
    "evaluation(y_test, y_pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC also didn't perform as well as Logistic Regression and Random Forest. It had an accuracy of 0.7839. However, it performed better than Naive Bayes. Thus, I will run some further iterations with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.268321Z",
     "start_time": "2020-12-17T17:15:36.254740Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest CV</th>\n",
       "      <td>0.802220</td>\n",
       "      <td>0.837754</td>\n",
       "      <td>0.766869</td>\n",
       "      <td>0.800746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accuracy  Precision    Recall  F1 Score\n",
       "Baseline Random Forest CV         0.802220   0.837754  0.766869  0.800746\n",
       "Baseline Logisitic Regression CV  0.798150   0.810458  0.796858  0.803600\n",
       "Baseline SVC CV                   0.783904   0.794234  0.786862  0.790531\n",
       "Baseline Naive Bayes CV           0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the models performed quite well for baseline models. The three best models were Random Forest, Logistic Regression, and SVC. Naive Bayes did a good job as well, but it under performed if compared to the other models in the accuracy metric, which is my focus for this project.\n",
    "\n",
    "It is interesting to see that Logistic Regression performed in second place between these baseline models. It tells us that although not usually seen as a powerful model, it holds it's surprises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will see how each of the three best models in the previous section performs when using the TF-IDF vectorizer. Too keep each to read, the evaluations will be at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.629885Z",
     "start_time": "2020-12-17T17:15:36.271238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lg_baseline = LogisticRegression()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lg_baseline.fit(X_train_tfidf, y_train) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lg_base_tfidf = lg_baseline.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.657860Z",
     "start_time": "2020-12-17T17:15:36.632517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lg_tfidf_accuracy = accuracy_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_precision = precision_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_recall = recall_score(y_test, y_lg_base_tfidf)\n",
    "lg_tfidf_f1 = f1_score(y_test, y_lg_base_tfidf)\n",
    "\n",
    "metric_dict['Baseline Logistic Regression TF-IDF'] = {'Accuracy': lg_tfidf_accuracy,\n",
    "                                                'Precision': lg_tfidf_precision,\n",
    "                                                'Recall': lg_tfidf_recall,\n",
    "                                                'F1 Score': lg_tfidf_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:36.687011Z",
     "start_time": "2020-12-17T17:15:36.661381Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8170212765957446\n",
      "Precision: 0.8481936971560338\n",
      "Recall: 0.7879328811138879\n",
      "F1 Score: 0.8169535443272257\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test, y_lg_base_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement with Logistic Regressing and TF-IDF. The accuracy improved from 0.7981 to 0.8170."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:58.047313Z",
     "start_time": "2020-12-17T17:15:36.691419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "rf_baseline = RandomForestClassifier()\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_base_tfidf = rf_baseline.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:58.067424Z",
     "start_time": "2020-12-17T17:15:58.049341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_tfidf_accuracy = accuracy_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_precision = precision_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_recall = recall_score(y_test, y_rf_base_tfidf)\n",
    "rf_tfidf_f1 = f1_score(y_test, y_rf_base_tfidf)\n",
    "\n",
    "metric_dict['Baseline Random Forest TF-IDF'] = {'Accuracy': rf_tfidf_accuracy,\n",
    "                                                'Precision': rf_tfidf_precision,\n",
    "                                                'Recall': rf_tfidf_recall,\n",
    "                                                'F1 Score': rf_tfidf_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:15:58.086517Z",
     "start_time": "2020-12-17T17:15:58.069245Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8035152636447733\n",
      "Precision: 0.8397811645173896\n",
      "Recall: 0.7672259907176009\n",
      "F1 Score: 0.801865671641791\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test, y_rf_base_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with TF-IDF also performed slightly better than when I tried it with Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T16:43:18.063837Z",
     "start_time": "2020-12-15T16:43:18.061209Z"
    }
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:17:38.412437Z",
     "start_time": "2020-12-17T17:15:58.088521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline SVC Model\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "svc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_pred_svc_tfidf = svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:17:38.435798Z",
     "start_time": "2020-12-17T17:17:38.414767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_tfidf_accuracy = accuracy_score(y_test, y_pred_svc_tfidf)\n",
    "svc_tfidf_precision = precision_score(y_test, y_pred_svc_tfidf)\n",
    "svc_tfidf_recall = recall_score(y_test, y_pred_svc_tfidf)\n",
    "svc_tfidf_f1 = f1_score(y_test, y_pred_svc_tfidf)\n",
    "\n",
    "metric_dict['Baseline SVC TF-IDF'] = {'Accuracy': svc_tfidf_accuracy,\n",
    "                                                'Precision': svc_tfidf_precision,\n",
    "                                                'Recall': svc_tfidf_recall,\n",
    "                                                'F1 Score': svc_tfidf_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:17:38.459059Z",
     "start_time": "2020-12-17T17:17:38.438931Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8222016651248844\n",
      "Precision: 0.8622047244094488\n",
      "Recall: 0.7818636201356658\n",
      "F1 Score: 0.8200711477251451\n"
     ]
    }
   ],
   "source": [
    "# SVC TF-IDF baseline evaluation\n",
    "evaluation(y_test, y_pred_svc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC had a good improvement in accuracy. It went from 0.7839 to 0.8222. It is the best model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:17:38.476266Z",
     "start_time": "2020-12-17T17:17:38.461938Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline SVC TF-IDF</th>\n",
       "      <td>0.822202</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.781864</td>\n",
       "      <td>0.820071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression TF-IDF</th>\n",
       "      <td>0.817021</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest TF-IDF</th>\n",
       "      <td>0.803515</td>\n",
       "      <td>0.839781</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>0.801866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest CV</th>\n",
       "      <td>0.802220</td>\n",
       "      <td>0.837754</td>\n",
       "      <td>0.766869</td>\n",
       "      <td>0.800746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Accuracy  Precision    Recall  F1 Score\n",
       "Baseline SVC TF-IDF                  0.822202   0.862205  0.781864  0.820071\n",
       "Baseline Logistic Regression TF-IDF  0.817021   0.848194  0.787933  0.816954\n",
       "Baseline Random Forest TF-IDF        0.803515   0.839781  0.767226  0.801866\n",
       "Baseline Random Forest CV            0.802220   0.837754  0.766869  0.800746\n",
       "Baseline Logisitic Regression CV     0.798150   0.810458  0.796858  0.803600\n",
       "Baseline SVC CV                      0.783904   0.794234  0.786862  0.790531\n",
       "Baseline Naive Bayes CV              0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all the models using TF-IDF had a better performance than the models using CountVectorizer. SVC model had an big improvement compared to other models and Logistic Regression was able to perform better than Random Forest, which is surprising. The model that had the lowest improvement was Random Forest, which we can see that it changed very little.\n",
    "\n",
    "I will now test these three models with the lemmatized dataset and see if there is any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF With Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try the three best models using the lemmatized variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:39:21.570320Z",
     "start_time": "2020-12-17T17:39:21.118879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Logistic Regression Model\n",
    "lr_baseline = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "lr_baseline.fit(X_train_lem, y_train_lem) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_lr_base_tfidf_lem = lr_baseline.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:39:22.090037Z",
     "start_time": "2020-12-17T17:39:22.072874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "lr_lem_accuracy = accuracy_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_precision = precision_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_recall = recall_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "lr_lem_f1 = f1_score(y_test_lem, y_lr_base_tfidf_lem)\n",
    "\n",
    "metric_dict['Baseline Logistic Regression Lem'] = {'Accuracy': lr_lem_accuracy,\n",
    "                                                'Precision': lr_lem_precision,\n",
    "                                                'Recall': lr_lem_recall,\n",
    "                                                'F1 Score': lr_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:39:22.654891Z",
     "start_time": "2020-12-17T17:39:22.638073Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8177613320999075\n",
      "Precision: 0.8314990512333966\n",
      "Recall: 0.8019765739385066\n",
      "F1 Score: 0.8164710266443078\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_lg_base_tfidf_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration performed slightly better than the iteration without lemmatization: 0.81777 vs 0.81770. However, it's insignificant. Thus, I'll consider  the precision metric to decide which model will move forward. In this case, it will be the model without lemmatizaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:18:00.334272Z",
     "start_time": "2020-12-17T17:17:38.988793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_baseline = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_baseline.fit(X_train_lem, y_train_lem) \n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_rf_base_lem = rf_baseline.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:18:00.357449Z",
     "start_time": "2020-12-17T17:18:00.336551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_lem_accuracy = accuracy_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_precision = precision_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_recall = recall_score(y_test_lem, y_rf_base_lem)\n",
    "rf_lem_f1 = f1_score(y_test_lem, y_rf_base_lem)\n",
    "\n",
    "metric_dict['Baseline Random Forest Lem'] = {'Accuracy': rf_lem_accuracy,\n",
    "                                                'Precision': rf_lem_precision,\n",
    "                                                'Recall': rf_lem_recall,\n",
    "                                                'F1 Score': rf_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:18:00.379565Z",
     "start_time": "2020-12-17T17:18:00.360111Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7988899167437558\n",
      "Precision: 0.8196657598134474\n",
      "Recall: 0.7719619326500732\n",
      "F1 Score: 0.7950989632422243\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_rf_base_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with lemmatization is the best model so far with an accuracy of 0.8231 and 0.8048 without lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:19:37.531643Z",
     "start_time": "2020-12-17T17:18:00.382293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline SVC Model\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "svc.fit(X_train_lem, y_train_lem)\n",
    "\n",
    "# Predicting the model in the X variable of the test set\n",
    "y_pred_svc_lem = svc.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:19:37.564363Z",
     "start_time": "2020-12-17T17:19:37.534312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_lem_accuracy = accuracy_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_precision = precision_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_recall = recall_score(y_test_lem, y_pred_svc_lem)\n",
    "svc_lem_f1 = f1_score(y_test_lem, y_pred_svc_lem)\n",
    "\n",
    "metric_dict['Baseline Random Forest Lem'] = {'Accuracy': svc_lem_accuracy,\n",
    "                                                'Precision': svc_lem_precision,\n",
    "                                                'Recall': svc_lem_recall,\n",
    "                                                'F1 Score': svc_lem_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:19:37.593862Z",
     "start_time": "2020-12-17T17:19:37.569908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8231267345050879\n",
      "Precision: 0.8493312352478364\n",
      "Recall: 0.7902635431918009\n",
      "F1 Score: 0.8187334091770953\n"
     ]
    }
   ],
   "source": [
    "# SCV TF-IDF baseline evaluation\n",
    "evaluation(y_test_lem, y_pred_svc_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:19:37.617631Z",
     "start_time": "2020-12-17T17:19:37.596551Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest Lem</th>\n",
       "      <td>0.823127</td>\n",
       "      <td>0.849331</td>\n",
       "      <td>0.790264</td>\n",
       "      <td>0.818733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC TF-IDF</th>\n",
       "      <td>0.822202</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.781864</td>\n",
       "      <td>0.820071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression Lem</th>\n",
       "      <td>0.817761</td>\n",
       "      <td>0.831499</td>\n",
       "      <td>0.801977</td>\n",
       "      <td>0.816471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression TF-IDF</th>\n",
       "      <td>0.817021</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest TF-IDF</th>\n",
       "      <td>0.803515</td>\n",
       "      <td>0.839781</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>0.801866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest CV</th>\n",
       "      <td>0.802220</td>\n",
       "      <td>0.837754</td>\n",
       "      <td>0.766869</td>\n",
       "      <td>0.800746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Accuracy  Precision    Recall  F1 Score\n",
       "Baseline Random Forest Lem           0.823127   0.849331  0.790264  0.818733\n",
       "Baseline SVC TF-IDF                  0.822202   0.862205  0.781864  0.820071\n",
       "Baseline Logistic Regression Lem     0.817761   0.831499  0.801977  0.816471\n",
       "Baseline Logistic Regression TF-IDF  0.817021   0.848194  0.787933  0.816954\n",
       "Baseline Random Forest TF-IDF        0.803515   0.839781  0.767226  0.801866\n",
       "Baseline Random Forest CV            0.802220   0.837754  0.766869  0.800746\n",
       "Baseline Logisitic Regression CV     0.798150   0.810458  0.796858  0.803600\n",
       "Baseline SVC CV                      0.783904   0.794234  0.786862  0.790531\n",
       "Baseline Naive Bayes CV              0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Random Forest is our best model so far compared to all the previous model. However, the baseline SVC model using TF-IDF is very close to it with a higher precision and higher F1 Score. They aren't my main focus when it comes to metrics, but I might take in consideration when choosing the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fixed the class imbalance manually. However, I'm curious to see how my best models will behave using SMOTE. For this reason I'll run two of my best models and see how it would perform with SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:19:44.263545Z",
     "start_time": "2020-12-17T17:19:37.622272Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train_smote_lem, y_train_smote_lem = smote.fit_sample(X_train_lem, y_train_lem) \n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_smote_tfidf, y_train_smote_tfidf = smote.fit_sample(X_train_tfidf, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:20:05.772155Z",
     "start_time": "2020-12-17T17:19:44.265600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating baseline Random Forest Model\n",
    "rf_base_lem_tfidf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fitting the model to the X and y variables of the train set\n",
    "rf_base_lem_tfidf.fit(X_train_smote_lem, y_train_smote_lem)\n",
    "\n",
    "# Predicting the model in the X variable of the test set### Random Forest\n",
    "y_rf_base_lem_tfidf_smote = rf_base_lem_tfidf.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:20:05.790741Z",
     "start_time": "2020-12-17T17:20:05.774229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "rf_smote_accuracy = accuracy_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_precision = precision_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_recall = recall_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "rf_smote_f1 = f1_score(y_test_lem, y_rf_base_lem_tfidf_smote)\n",
    "\n",
    "metric_dict['Baseline Random Forest SMOTE'] = {'Accuracy': rf_smote_accuracy,\n",
    "                                                'Precision': rf_smote_precision,\n",
    "                                                'Recall': rf_smote_recall,\n",
    "                                                'F1 Score': rf_smote_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:20:05.809721Z",
     "start_time": "2020-12-17T17:20:05.792786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.7994449583718779\n",
      "Precision: 0.8293365307753797\n",
      "Recall: 0.7595168374816984\n",
      "F1 Score: 0.7928926251432938\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test_lem, y_rf_base_lem_tfidf_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest did not perform any better using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T19:40:32.733033Z",
     "start_time": "2020-12-16T19:40:32.730826Z"
    }
   },
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.663435Z",
     "start_time": "2020-12-17T17:20:05.811839Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train_smote_tfidf, y_train_smote_tfidf)\n",
    "y_pred_svc_smote = svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.681781Z",
     "start_time": "2020-12-17T17:21:50.665628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with all metrics\n",
    "svc_smote_accuracy = accuracy_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_precision = precision_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_recall = recall_score(y_test, y_pred_svc_smote)\n",
    "svc_smote_f1 = f1_score(y_test, y_pred_svc_smote)\n",
    "\n",
    "metric_dict['Baseline SVC SMOTE'] = {'Accuracy': svc_smote_accuracy,\n",
    "                                                'Precision': svc_smote_precision,\n",
    "                                                'Recall': svc_smote_recall,\n",
    "                                                'F1 Score': svc_smote_f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.701232Z",
     "start_time": "2020-12-17T17:21:50.683933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8238667900092507\n",
      "Precision: 0.8664288545382481\n",
      "Recall: 0.7804355587290254\n",
      "F1 Score: 0.8211870773854245\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_test, y_pred_svc_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.718805Z",
     "start_time": "2020-12-17T17:21:50.703661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline SVC SMOTE</th>\n",
       "      <td>0.823867</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.780436</td>\n",
       "      <td>0.821187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest Lem</th>\n",
       "      <td>0.823127</td>\n",
       "      <td>0.849331</td>\n",
       "      <td>0.790264</td>\n",
       "      <td>0.818733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC TF-IDF</th>\n",
       "      <td>0.822202</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.781864</td>\n",
       "      <td>0.820071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression Lem</th>\n",
       "      <td>0.817761</td>\n",
       "      <td>0.831499</td>\n",
       "      <td>0.801977</td>\n",
       "      <td>0.816471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression TF-IDF</th>\n",
       "      <td>0.817021</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest TF-IDF</th>\n",
       "      <td>0.803515</td>\n",
       "      <td>0.839781</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>0.801866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest CV</th>\n",
       "      <td>0.802220</td>\n",
       "      <td>0.837754</td>\n",
       "      <td>0.766869</td>\n",
       "      <td>0.800746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest SMOTE</th>\n",
       "      <td>0.799445</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.759517</td>\n",
       "      <td>0.792893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Accuracy  Precision    Recall  F1 Score\n",
       "Baseline SVC SMOTE                   0.823867   0.866429  0.780436  0.821187\n",
       "Baseline Random Forest Lem           0.823127   0.849331  0.790264  0.818733\n",
       "Baseline SVC TF-IDF                  0.822202   0.862205  0.781864  0.820071\n",
       "Baseline Logistic Regression Lem     0.817761   0.831499  0.801977  0.816471\n",
       "Baseline Logistic Regression TF-IDF  0.817021   0.848194  0.787933  0.816954\n",
       "Baseline Random Forest TF-IDF        0.803515   0.839781  0.767226  0.801866\n",
       "Baseline Random Forest CV            0.802220   0.837754  0.766869  0.800746\n",
       "Baseline Random Forest SMOTE         0.799445   0.829337  0.759517  0.792893\n",
       "Baseline Logisitic Regression CV     0.798150   0.810458  0.796858  0.803600\n",
       "Baseline SVC CV                      0.783904   0.794234  0.786862  0.790531\n",
       "Baseline Naive Bayes CV              0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the SMOTE was able to improve the accuracy metric by a very small different. However, Recall and F1 Score dropped, which tells me that the model is not as good as the version without SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T19:47:12.077415Z",
     "start_time": "2020-12-16T19:47:12.075016Z"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.734763Z",
     "start_time": "2020-12-17T17:21:50.721387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline SVC SMOTE</th>\n",
       "      <td>0.823867</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.780436</td>\n",
       "      <td>0.821187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest Lem</th>\n",
       "      <td>0.823127</td>\n",
       "      <td>0.849331</td>\n",
       "      <td>0.790264</td>\n",
       "      <td>0.818733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC TF-IDF</th>\n",
       "      <td>0.822202</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.781864</td>\n",
       "      <td>0.820071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression Lem</th>\n",
       "      <td>0.817761</td>\n",
       "      <td>0.831499</td>\n",
       "      <td>0.801977</td>\n",
       "      <td>0.816471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logistic Regression TF-IDF</th>\n",
       "      <td>0.817021</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.816954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest TF-IDF</th>\n",
       "      <td>0.803515</td>\n",
       "      <td>0.839781</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>0.801866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest CV</th>\n",
       "      <td>0.802220</td>\n",
       "      <td>0.837754</td>\n",
       "      <td>0.766869</td>\n",
       "      <td>0.800746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Random Forest SMOTE</th>\n",
       "      <td>0.799445</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.759517</td>\n",
       "      <td>0.792893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression CV</th>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVC CV</th>\n",
       "      <td>0.783904</td>\n",
       "      <td>0.794234</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.790531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline Naive Bayes CV</th>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.793541</td>\n",
       "      <td>0.780793</td>\n",
       "      <td>0.787115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Accuracy  Precision    Recall  F1 Score\n",
       "Baseline SVC SMOTE                   0.823867   0.866429  0.780436  0.821187\n",
       "Baseline Random Forest Lem           0.823127   0.849331  0.790264  0.818733\n",
       "Baseline SVC TF-IDF                  0.822202   0.862205  0.781864  0.820071\n",
       "Baseline Logistic Regression Lem     0.817761   0.831499  0.801977  0.816471\n",
       "Baseline Logistic Regression TF-IDF  0.817021   0.848194  0.787933  0.816954\n",
       "Baseline Random Forest TF-IDF        0.803515   0.839781  0.767226  0.801866\n",
       "Baseline Random Forest CV            0.802220   0.837754  0.766869  0.800746\n",
       "Baseline Random Forest SMOTE         0.799445   0.829337  0.759517  0.792893\n",
       "Baseline Logisitic Regression CV     0.798150   0.810458  0.796858  0.803600\n",
       "Baseline SVC CV                      0.783904   0.794234  0.786862  0.790531\n",
       "Baseline Naive Bayes CV              0.781129   0.793541  0.780793  0.787115"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "evaluation_df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running multiple baseline models, we were able to verify a few points:\n",
    "- Our best model was a Random Forest using lemmatized words\n",
    "- Random Forest, CVC, and Logistic Regression are the best models to use in NLP in these cases\n",
    "- TF-IDF was able improve all all our models.\n",
    "- Naive Bayes was our worst model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with my best models in mind, I will run a few ensemble models in the <a href=\"https://github.com/Ismaeltrevi/hotel-reviews-analysis-using-nlp/blob/main/models/ensemble-models.ipynb\">Data Cleaning</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.742598Z",
     "start_time": "2020-12-17T17:21:50.737386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/vanilla_model_evaluation.pkl\",'wb')\n",
    "pickle.dump(evaluation_df, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.771863Z",
     "start_time": "2020-12-17T17:21:50.745243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/X_train_tfidf.pkl\",'wb')\n",
    "pickle.dump(X_train_tfidf, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.801701Z",
     "start_time": "2020-12-17T17:21:50.774196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Test Set - Features\n",
    "pickle_out = open(\"../pickle/X_test_tfidf.pkl\",'wb')\n",
    "pickle.dump(X_test_tfidf, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.812173Z",
     "start_time": "2020-12-17T17:21:50.804013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train and Test Set  Target\n",
    "y_train.to_pickle(\"../pickle/y_train.pkl\")\n",
    "y_test.to_pickle(\"../pickle/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.836071Z",
     "start_time": "2020-12-17T17:21:50.814488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train Set - Features\n",
    "pickle_out = open(\"../pickle/X_train_lem.pkl\",'wb')\n",
    "pickle.dump(X_train_lem, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.847829Z",
     "start_time": "2020-12-17T17:21:50.838224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Test Set - Features\n",
    "pickle_out = open(\"../pickle/X_test_lem.pkl\",'wb')\n",
    "pickle.dump(X_test_lem, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T17:21:50.856200Z",
     "start_time": "2020-12-17T17:21:50.849921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickling Train and Test Set - Target\n",
    "y_train_lem.to_pickle(\"../pickle/y_train_lem.pkl\")\n",
    "y_test_lem.to_pickle(\"../pickle/y_test_lem.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
